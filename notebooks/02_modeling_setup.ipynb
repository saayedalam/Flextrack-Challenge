{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6b2334",
   "metadata": {},
   "source": [
    "# ðŸ§© FlexTrack Challenge â€” Modeling Pipeline Setup\n",
    "\n",
    "**Notebook:** 02_modeling_setup.ipynb  \n",
    "**Goal:** Build a reliable Phase 1 classification pipeline with time-aware, site-aware cross-validation and feature-rich LightGBM modeling.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Objectives\n",
    "- Implement **cross-validation strategies** that prevent leakage:\n",
    "  - Time-series folds (per-site, chronological)\n",
    "  - Leave-one-site-out (LOSO) for robustness checks\n",
    "- Validate split integrity (no future data leakage, no site overlap)\n",
    "- Train and evaluate a **LightGBM classifier** with imbalance handling\n",
    "- Establish the foundation for:\n",
    "  - Threshold tuning\n",
    "  - Feature engineering iterations\n",
    "  - Phase 2 regression (capacity prediction)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Context Recap\n",
    "- **Phase 1 Target:** `Demand_Response_Flag` (0 = no event, Â±1 = event)  \n",
    "- **Phase 2 Target:** `Demand_Response_Capacity_kW` (continuous, only when event â‰  0)  \n",
    "- **Key EDA insights:**  \n",
    "  - Extreme class imbalance (~97% zeros)  \n",
    "  - Events mostly during business hours, hot/sunny conditions  \n",
    "  - Building power highly autocorrelated (lags/rolling stats are predictive)  \n",
    "- **Feature blueprint:**  \n",
    "  - Time-based (hour, weekday, month, business hours, weekend)\n",
    "  - Weather (temperature, radiation, interactions)\n",
    "  - Load dynamics (lags, rolling means/std, deltas, z-scores)\n",
    "  - Site identity (one-hot encoding)\n",
    "  - Interaction terms (e.g. temp Ã— radiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7511fd",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup & Imports\n",
    "Minimal, reproducible environment for Phase 1 modeling:\n",
    "- Core: NumPy, Pandas, Matplotlib\n",
    "- Model: LightGBM (sklearn API)\n",
    "- CV & Metrics: TimeSeriesSplit, confusion_matrix\n",
    "- Quiet warnings for clean logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3874cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# === Core ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# === Modeling ===\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# === CV & Metrics ===\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# === Misc ===\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Environment ready. Seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6724b2",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Load Training & Test Data\n",
    "We load both the processed **training data** and the official **public test set**.  \n",
    "- `Timestamp_Local` is parsed as datetime for time-based validation and feature engineering.  \n",
    "- Shapes are logged for confirmation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3603a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (105120, 7)\n",
      "Test shape: (35040, 5)\n"
     ]
    }
   ],
   "source": [
    "# --- Load train & test data ---\n",
    "train_path = \"../data/raw/flextrack-training-data-v0.1.csv\"\n",
    "test_path  = \"../data/raw/flextrack-public-test-data-v0.1.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path, parse_dates=[\"Timestamp_Local\"])\n",
    "df_test  = pd.read_csv(test_path,  parse_dates=[\"Timestamp_Local\"])\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f6a95",
   "metadata": {},
   "source": [
    "## â³ Cross-Validation Strategy\n",
    "\n",
    "We define a **time-aware, site-aware cross-validation splitter**.  \n",
    "- Splits are created **independently within each site**, preserving temporal order.  \n",
    "- Then, the site-level splits are combined to form global folds.  \n",
    "- This ensures no leakage across time or sites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8f9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_site_timeseries_folds(df, n_splits=3):\n",
    "    \"\"\"\n",
    "    Create time-aware, site-aware CV folds.\n",
    "\n",
    "    Each site is split independently using TimeSeriesSplit,\n",
    "    then site-level splits are combined into global folds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe sorted by ['Site', 'Timestamp_Local'].\n",
    "    n_splits : int, default=3\n",
    "        Number of folds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folds : list of tuple\n",
    "        List of (train_idx, val_idx) index arrays.\n",
    "    \"\"\"\n",
    "    folds_per_site = {}\n",
    "    for site, dsite in df.groupby(\"Site\", sort=False):\n",
    "        idx = dsite.index.to_numpy()\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        site_folds = [(idx[tr_rel], idx[va_rel]) for tr_rel, va_rel in tscv.split(idx)]\n",
    "        folds_per_site[site] = site_folds\n",
    "\n",
    "    folds = []\n",
    "    for k in range(n_splits):\n",
    "        tr_parts, va_parts = [], []\n",
    "        for site in folds_per_site:\n",
    "            tr_idx, va_idx = folds_per_site[site][k]\n",
    "            tr_parts.append(tr_idx)\n",
    "            va_parts.append(va_idx)\n",
    "        folds.append((np.concatenate(tr_parts), np.concatenate(va_parts)))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0d781",
   "metadata": {},
   "source": [
    "## ðŸŒ Leave-One-Site-Out (LOSO) Folds\n",
    "\n",
    "As a **robust generalization check**, we build folds where:  \n",
    "- Training happens on two sites  \n",
    "- Validation happens on the **held-out third site**  \n",
    "\n",
    "This mimics the scenario of applying the model to an unseen site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845b814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_leave_one_site_out_folds(df):\n",
    "    \"\"\"\n",
    "    Create leave-one-site-out (LOSO) folds.\n",
    "\n",
    "    For each site, train on all other sites and validate on the held-out site.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with a 'Site' column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folds : list of tuple\n",
    "        List of (train_idx, val_idx) index arrays.\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    all_idx = df.index.to_numpy()\n",
    "\n",
    "    for site, dsite in df.groupby(\"Site\", sort=False):\n",
    "        val_idx = dsite.index.to_numpy()\n",
    "        train_mask = np.isin(all_idx, val_idx, invert=True)\n",
    "        tr_idx = all_idx[train_mask]\n",
    "        folds.append((tr_idx, val_idx))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20817d",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Fold Integrity Check\n",
    "\n",
    "To avoid **temporal leakage**, we define a helper that:  \n",
    "- Summarizes the row counts and time ranges for train/valid splits  \n",
    "- Verifies that, within each site, the **training period ends before validation begins**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cfae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_fold(df, tr_idx, va_idx, label=\"\"):\n",
    "    \"\"\"\n",
    "    Summarize and validate the integrity of a single train/validation split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe containing 'Site' and 'Timestamp_Local'.\n",
    "    tr_idx : array-like\n",
    "        Row indices for the training set.\n",
    "    va_idx : array-like\n",
    "        Row indices for the validation set.\n",
    "    label : str, optional\n",
    "        Name/label for this fold (for printing).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If any site's training period overlaps with its validation period.\n",
    "    \"\"\"\n",
    "    tr, va = df.loc[tr_idx], df.loc[va_idx]\n",
    "\n",
    "    print(f\"{label} | Train: {tr.shape[0]:>6} rows  [{tr['Timestamp_Local'].min()} â†’ {tr['Timestamp_Local'].max()}]\")\n",
    "    print(f\"{' '*len(label)} | Valid: {va.shape[0]:>6} rows  [{va['Timestamp_Local'].min()} â†’ {va['Timestamp_Local'].max()}]\")\n",
    "\n",
    "    # ðŸš¨ Check for leakage per site\n",
    "    for site in df['Site'].unique():\n",
    "        tr_s, va_s = tr[tr['Site'] == site], va[va['Site'] == site]\n",
    "        if not tr_s.empty and not va_s.empty:\n",
    "            assert tr_s['Timestamp_Local'].max() < va_s['Timestamp_Local'].min(), f\"Temporal leakage detected in site {site}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728cfa1",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Cross-Validation Setup\n",
    "\n",
    "To ensure robust evaluation, we prepare two complementary CV strategies:\n",
    "\n",
    "- **TimeSeriesSplit (per-site):**  \n",
    "  Splits each site into time-ordered chunks to prevent temporal leakage.  \n",
    "  These are combined across sites to form 3 primary folds.\n",
    "\n",
    "- **Leave-One-Site-Out (LOSO):**  \n",
    "  Trains on two sites and validates on the third.  \n",
    "  This serves as a stricter robustness check for site generalization.\n",
    "\n",
    "Both folds are validated with integrity checks to confirm no overlap or leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3b22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build CV folds (primary + robustness) ---\n",
    "ts_folds   = make_site_timeseries_folds(df_train, n_splits=3)   # primary CV\n",
    "loso_folds = make_leave_one_site_out_folds(df_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe7316",
   "metadata": {},
   "source": [
    "## âœ… Cross-Validation Integrity Check\n",
    "\n",
    "The folds were successfully built and verified:\n",
    "\n",
    "- **TimeSeriesSplit (3 folds):**  \n",
    "  - Each fold uses past data for training and later periods for validation.  \n",
    "  - Train/valid boundaries respect chronological order and prevent leakage.  \n",
    "  - Each validation set has ~26k rows (â‰ˆ 25% of site data).  \n",
    "\n",
    "- **Leave-One-Site-Out (LOSO, 3 folds):**  \n",
    "  - Each fold trains on two sites and validates on the held-out site.  \n",
    "  - Ensures robustness to site-level distribution shifts.  \n",
    "  - Each validation set has 35k rows (one full site).  \n",
    "\n",
    "All checks confirmed no overlap or leakage between training and validation ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c2a4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS Fold 1 | Train:  26280 rows  [2019-01-01 00:00:00 â†’ 2023-04-02 05:45:00]\n",
      "          | Valid:  26280 rows  [2019-04-02 06:00:00 â†’ 2023-07-02 11:45:00]\n",
      "TS Fold 2 | Train:  52560 rows  [2019-01-01 00:00:00 â†’ 2023-07-02 11:45:00]\n",
      "          | Valid:  26280 rows  [2019-07-02 12:00:00 â†’ 2023-10-01 17:45:00]\n",
      "TS Fold 3 | Train:  78840 rows  [2019-01-01 00:00:00 â†’ 2023-10-01 17:45:00]\n",
      "          | Valid:  26280 rows  [2019-10-01 18:00:00 â†’ 2023-12-31 23:45:00]\n",
      "LOSO Fold 1 | Train:  70080 rows  [2019-01-01 00:00:00 â†’ 2023-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2019-01-01 00:00:00 â†’ 2019-12-31 23:45:00]\n",
      "LOSO Fold 2 | Train:  70080 rows  [2019-01-01 00:00:00 â†’ 2023-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2019-01-01 00:00:00 â†’ 2019-12-31 23:45:00]\n",
      "LOSO Fold 3 | Train:  70080 rows  [2019-01-01 00:00:00 â†’ 2019-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2023-01-01 00:00:00 â†’ 2023-12-31 23:45:00]\n"
     ]
    }
   ],
   "source": [
    "# --- Integrity summaries ---\n",
    "for i, (tr_idx, va_idx) in enumerate(ts_folds, 1):\n",
    "    summarize_fold(df_train, tr_idx, va_idx, label=f\"TS Fold {i}\")\n",
    "for i, (tr_idx, va_idx) in enumerate(loso_folds, 1):\n",
    "    summarize_fold(df_train, tr_idx, va_idx, label=f\"LOSO Fold {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26f7b1",
   "metadata": {},
   "source": [
    "## ðŸ“Š G-Mean from Hard Predictions\n",
    "\n",
    "To handle **class imbalance**, we compute the geometric mean (G-Mean) of sensitivity and specificity.  \n",
    "This helper:  \n",
    "- Derives TPR (recall for events) and TNR (specificity for non-events)  \n",
    "- Returns G-Mean = âˆš(TPR Â· TNR) along with both components  \n",
    "- Works directly on **hard 0/1 predictions**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec2e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean_binary(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute G-Mean, TPR, and TNR from binary predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground-truth labels (0 = non-event, 1 = event).\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Hard predictions (0/1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gmean : float\n",
    "        Geometric mean of TPR and TNR.\n",
    "    tpr : float\n",
    "        True Positive Rate (recall for the positive class).\n",
    "    tnr : float\n",
    "        True Negative Rate (specificity for the negative class).\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    return float((tpr * tnr) ** 0.5), float(tpr), float(tnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ceb991",
   "metadata": {},
   "source": [
    "## ðŸ“Š G-Mean from Probability Thresholds\n",
    "\n",
    "To better handle class imbalance, we sweep across probability thresholds:  \n",
    "- Convert probabilities into hard 0/1 predictions at each threshold  \n",
    "- Compute **G-Mean, TPR, and TNR**  \n",
    "- Select the threshold that maximizes G-Mean  \n",
    "\n",
    "This avoids relying on the default `0.5` cutoff, which is often poor for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa16a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean_from_probs(y_true, y_prob, thresholds):\n",
    "    \"\"\"\n",
    "    Sweep thresholds to maximize G-Mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground-truth labels (0 = non-event, 1 = event).\n",
    "    y_prob : array-like of shape (n_samples,)\n",
    "        Predicted probabilities for the positive class.\n",
    "    thresholds : array-like\n",
    "        Candidate thresholds to evaluate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_gm : float\n",
    "        Best geometric mean of TPR and TNR across thresholds.\n",
    "    best_thr : float\n",
    "        Threshold that achieved the best G-Mean.\n",
    "    best_tpr : float\n",
    "        True Positive Rate (recall for the positive class) at best threshold.\n",
    "    best_tnr : float\n",
    "        True Negative Rate (specificity for the negative class) at best threshold.\n",
    "    \"\"\"\n",
    "    best = (-1.0, 0.5, 0.0, 0.0)  # (gm, thr, tpr, tnr)\n",
    "\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp+fn) > 0 else 0.0\n",
    "        tnr = tn / (tn + fp) if (tn+fp) > 0 else 0.0\n",
    "        gm  = np.sqrt(tpr * tnr)\n",
    "\n",
    "        if gm > best[0]:\n",
    "            best = (gm, thr, tpr, tnr)\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b72906",
   "metadata": {},
   "source": [
    "## ðŸ§± Feature Engineering\n",
    "\n",
    "Build leakage-safe features capturing **time**, **weather**, and **load dynamics**:\n",
    "\n",
    "- **Time**: `hour`, `dow`, `month`, `bizhrs` (10â€“17), `wknd` (Sat/Sun).\n",
    "- **Weather**: `temp` (Dry_Bulb_Temperature_C), `rad` (Global_Horizontal_Radiation_W/m2), `temp_x_rad` (interaction).\n",
    "- **Load lags (per-site)**: `power_lag1`, `power_lag4`, `power_lag12`, `power_lag24`, `power_lag96`.\n",
    "- **Rolling stats (per-site)**: `power_roll4`, `power_roll12`, `power_roll96`, `power_std96`, `power_min96`, `power_max96`.\n",
    "- **Daily context**: `power_dev96` (current âˆ’ 24h mean), `power_rel96` (current Ã· 24h mean).\n",
    "- **Change & scale**: `power_delta` (diff vs lag1), `power_z` (per-site z-score).\n",
    "- **Site identity**: one-hot `Site_*`.\n",
    "- **Target**: `y_event = 1` if `Demand_Response_Flag != 0` else `0` (if available; `NaN` for test).\n",
    "\n",
    "_All rolling/lag features are computed within each site and only use past data to avoid temporal leakage._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a378c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct model features from raw FlexTrack columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain at least:\n",
    "        - 'Site', 'Timestamp_Local', 'Building_Power_kW',\n",
    "        - 'Dry_Bulb_Temperature_C', 'Global_Horizontal_Radiation_W/m2'\n",
    "        Optionally: 'Demand_Response_Flag' (if present, y_event is derived).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input with engineered features and one-hot 'Site_*' columns.\n",
    "        If 'Demand_Response_Flag' exists, includes binary target 'y_event'.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    # --- datetime ordering ---\n",
    "    d = d.sort_values([\"Site\", \"Timestamp_Local\"])\n",
    "\n",
    "    # --- target (if available) ---\n",
    "    if \"Demand_Response_Flag\" in d.columns:\n",
    "        d[\"y_event\"] = (d[\"Demand_Response_Flag\"] != 0).astype(int)\n",
    "    else:\n",
    "        d[\"y_event\"] = np.nan\n",
    "\n",
    "    # --- time features ---\n",
    "    d[\"hour\"]   = d[\"Timestamp_Local\"].dt.hour\n",
    "    d[\"dow\"]    = d[\"Timestamp_Local\"].dt.dayofweek\n",
    "    d[\"month\"]  = d[\"Timestamp_Local\"].dt.month\n",
    "    d[\"bizhrs\"] = d[\"hour\"].between(10, 17).astype(int)\n",
    "    d[\"wknd\"]   = d[\"dow\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    # --- weather features ---\n",
    "    d[\"temp\"] = d[\"Dry_Bulb_Temperature_C\"]\n",
    "    d[\"rad\"]  = d[\"Global_Horizontal_Radiation_W/m2\"]\n",
    "    d[\"temp_x_rad\"] = d[\"temp\"] * d[\"rad\"]\n",
    "\n",
    "    # --- load lags (per-site) ---\n",
    "    for lag in [1, 4, 12, 24, 96]:\n",
    "        d[f\"power_lag{lag}\"] = d.groupby(\"Site\", sort=False)[\"Building_Power_kW\"].shift(lag)\n",
    "\n",
    "    # --- rolling stats (per-site) ---\n",
    "    grp = d.groupby(\"Site\", sort=False)[\"Building_Power_kW\"]\n",
    "    d[\"power_roll4\"]   = grp.transform(lambda s: s.rolling(4,  min_periods=1).mean())\n",
    "    d[\"power_roll12\"]  = grp.transform(lambda s: s.rolling(12, min_periods=1).mean())\n",
    "    d[\"power_roll96\"]  = grp.transform(lambda s: s.rolling(96, min_periods=1).mean())\n",
    "    d[\"power_std96\"]   = grp.transform(lambda s: s.rolling(96, min_periods=10).std())\n",
    "    d[\"power_min96\"]   = grp.transform(lambda s: s.rolling(96, min_periods=10).min())\n",
    "    d[\"power_max96\"]   = grp.transform(lambda s: s.rolling(96, min_periods=10).max())\n",
    "\n",
    "    # --- deviations / ratios / deltas ---\n",
    "    eps = 1e-6\n",
    "    d[\"power_dev96\"] = d[\"Building_Power_kW\"] - d[\"power_roll96\"]\n",
    "    d[\"power_rel96\"] = d[\"Building_Power_kW\"] / (d[\"power_roll96\"] + eps)\n",
    "    d[\"power_delta\"] = d[\"Building_Power_kW\"] - d[\"power_lag1\"]\n",
    "\n",
    "    # --- per-site z-score of power ---\n",
    "    d[\"power_z\"] = d.groupby(\"Site\", sort=False)[\"Building_Power_kW\"].transform(\n",
    "        lambda s: (s - s.mean()) / (s.std() + 1e-6)\n",
    "    )\n",
    "\n",
    "    # --- one-hot site ---\n",
    "    d = pd.get_dummies(d, columns=[\"Site\"], drop_first=False)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601766cd",
   "metadata": {},
   "source": [
    "## ðŸ§± Build Features (Train & Test)\n",
    "\n",
    "To ensure consistency across both splits, we apply the unified `build_features()` pipeline to train and test data.  \n",
    "\n",
    "This step:  \n",
    "- Generates **time, weather, and load-derived features**  \n",
    "- Adds **site one-hot encodings**  \n",
    "- Creates the `y_event` target for training data (left as `NaN` for test)  \n",
    "- Guarantees no leakage by only using **current or past values** (lags/rollings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2087b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build engineered features for train & test ---\n",
    "df_feat_train = build_features(df_train)\n",
    "df_feat_test  = build_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d5eba",
   "metadata": {},
   "source": [
    "## ðŸ”„ Align One-Hot Encoded Site Columns\n",
    "\n",
    "After one-hot encoding `Site`, the **train and test sets may not have identical dummy columns**:  \n",
    "- If a site is missing in the test set, its dummy column wonâ€™t be created.  \n",
    "- If an unexpected site appears in the test set, it wonâ€™t exist in the training set.  \n",
    "\n",
    "To prevent feature mismatches at inference time, we:  \n",
    "- Collect the list of all `Site_*` columns in both train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e73edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collect site dummy columns in train & test ---\n",
    "train_sites = [c for c in df_feat_train.columns if c.startswith(\"Site_\")]\n",
    "test_sites  = [c for c in df_feat_test.columns  if c.startswith(\"Site_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfda0e",
   "metadata": {},
   "source": [
    "## ðŸ§© Ensure Feature Consistency Between Train and Test\n",
    "\n",
    "To guarantee both datasets have the **same set of `Site_*` dummy columns**:  \n",
    "- Add any missing site columns in the test set and fill them with `0`  \n",
    "- Drop any unexpected site columns in the test set (safety check)  \n",
    "\n",
    "This ensures the **train and test feature matrices are aligned** before modeling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae25a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure feature consistency between train & test ---\n",
    "missing_sites = set(train_sites) - set(test_sites)\n",
    "extra_sites   = set(test_sites) - set(train_sites)\n",
    "\n",
    "# Add missing site dummies to test (fill with 0)\n",
    "for col in missing_sites:\n",
    "    df_feat_test[col] = 0\n",
    "\n",
    "# Drop extra site dummies from test (safety check)\n",
    "if extra_sites:\n",
    "    df_feat_test.drop(columns=list(extra_sites), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867dc26",
   "metadata": {},
   "source": [
    "## ðŸ§® Define Final Feature Set\n",
    "\n",
    "We explicitly select the features created in `build_features`:  \n",
    "- **Time features** â†’ `hour`, `dow`, `month`, `bizhrs`, `wknd`  \n",
    "- **Weather features** â†’ `temp`, `rad`, `temp_x_rad`  \n",
    "- **Load lags** â†’ `power_lag1`, `power_lag4`, `power_lag12`, `power_lag24`, `power_lag96`  \n",
    "- **Rolling statistics** â†’ `power_roll4`, `power_roll12`, `power_roll96`, `power_std96`  \n",
    "- **Daily context** â†’ `power_min96`, `power_max96`, `power_dev96`, `power_rel96`  \n",
    "- **Dynamics** â†’ `power_delta`, `power_z`  \n",
    "- **Site identity** â†’ all `Site_*` dummy variables  \n",
    "\n",
    "Finally, we restrict the list to columns that exist in **both train and test** so that the feature matrices remain aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3358d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 26\n"
     ]
    }
   ],
   "source": [
    "# --- Define feature list to match `build_features` ---\n",
    "feature_cols = [\n",
    "    # time\n",
    "    \"hour\", \"dow\", \"month\", \"bizhrs\", \"wknd\",\n",
    "\n",
    "    # weather\n",
    "    \"temp\", \"rad\", \"temp_x_rad\",\n",
    "\n",
    "    # load lags\n",
    "    \"power_lag1\", \"power_lag4\", \"power_lag12\", \"power_lag24\", \"power_lag96\",\n",
    "\n",
    "    # rolling stats (24h window = 96 steps)\n",
    "    \"power_roll4\", \"power_roll12\", \"power_roll96\", \"power_std96\",\n",
    "    \"power_min96\", \"power_max96\",\n",
    "\n",
    "    # deviations / ratios / deltas / normalization\n",
    "    \"power_dev96\", \"power_rel96\", \"power_delta\", \"power_z\",\n",
    "] + [c for c in df_feat_train.columns if c.startswith(\"Site_\")]\n",
    "\n",
    "# --- Ensure only features present in BOTH train & test ---\n",
    "feature_cols_final = [\n",
    "    c for c in feature_cols\n",
    "    if c in df_feat_train.columns and c in df_feat_test.columns\n",
    "]\n",
    "\n",
    "print(\"Num features:\", len(feature_cols_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d715648",
   "metadata": {},
   "source": [
    "## ðŸ§ª Time-Aware CV with Threshold Tuning\n",
    "\n",
    "We evaluate models using **time- and site-aware folds**, combined with a **threshold sweep** to handle class imbalance:\n",
    "\n",
    "- Train LightGBM on each fold with early stopping  \n",
    "- Sweep probability thresholds to maximize **G-Mean**  \n",
    "- Record the best threshold and per-fold metrics  \n",
    "\n",
    "This provides a **leakage-safe validation framework** and a principled way to choose cutoffs instead of relying on the default `0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09951ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_threshold_tuned(df_feat, folds, feature_cols, thresholds=None):\n",
    "    \"\"\"\n",
    "    Run time-aware CV with LightGBM and per-fold threshold tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_feat : pd.DataFrame\n",
    "        Feature matrix including target `y_event`.\n",
    "    folds : list[tuple]\n",
    "        List of (train_idx, val_idx) index splits.\n",
    "    feature_cols : list[str]\n",
    "        Columns used for training.\n",
    "    thresholds : array-like, optional\n",
    "        Candidate probability thresholds. Default: np.linspace(0.02, 0.5, 25).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : list[float]\n",
    "        Best G-Mean per fold.\n",
    "    chosen_thrs : list[float]\n",
    "        Best threshold per fold.\n",
    "    per_fold_stats : list[tuple]\n",
    "        (gmean, thr, tpr, tnr, pos, neg, pos_ratio) per fold.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.02, 0.5, 25)\n",
    "\n",
    "    scores, chosen_thrs, per_fold_stats = [], [], []\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        tr = df_feat.loc[tr_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "        va = df_feat.loc[va_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "\n",
    "        X_tr, y_tr = tr[feature_cols], tr[\"y_event\"]\n",
    "        X_va, y_va = va[feature_cols], va[\"y_event\"]\n",
    "\n",
    "        # Imbalance handling: sqrt(neg/pos)\n",
    "        pos = max((y_tr == 1).sum(), 1)\n",
    "        neg = (y_tr == 0).sum()\n",
    "        spw = (neg / pos) ** 0.5\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=127,\n",
    "            max_depth=-1,\n",
    "            min_data_in_leaf=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            scale_pos_weight=float(spw),\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=150, verbose=False),\n",
    "            lgb.log_evaluation(period=0)  # silence eval logging\n",
    "        ]\n",
    "\n",
    "        clf.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Probabilities (use best iteration if available)\n",
    "        try:\n",
    "            y_prob = clf.predict_proba(X_va, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        except Exception:\n",
    "            y_prob = clf.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        # Threshold sweep\n",
    "        best_gm, best_thr, best_tpr, best_tnr = gmean_from_probs(y_va, y_prob, thresholds)\n",
    "\n",
    "        pos_val = int((y_va == 1).sum())\n",
    "        neg_val = int((y_va == 0).sum())\n",
    "        pos_ratio = pos_val / max(len(y_va), 1)\n",
    "\n",
    "        scores.append(best_gm)\n",
    "        chosen_thrs.append(best_thr)\n",
    "        per_fold_stats.append((best_gm, best_thr, best_tpr, best_tnr, pos_val, neg_val, pos_ratio))\n",
    "\n",
    "        bi = getattr(clf, \"best_iteration_\", None)\n",
    "        print(\n",
    "            f\"Fold {i}  | GM={best_gm:.3f} | Thr={best_thr:.3f} | \"\n",
    "            f\"TPR={best_tpr:.3f} | TNR={best_tnr:.3f} | \"\n",
    "            f\"Pos={pos_val}, Neg={neg_val}, PosRatio={pos_ratio:.3%} \"\n",
    "            + (f\"| BestIter={bi}\" if bi is not None else \"\")\n",
    "        )\n",
    "\n",
    "    avg_pos_ratio = float(np.mean([s[6] for s in per_fold_stats])) if per_fold_stats else 0.0\n",
    "    print(f\"\\nCV G-Mean: {np.mean(scores):.3f} Â± {np.std(scores):.3f}\")\n",
    "    print(f\"Avg validation positive ratio: {avg_pos_ratio:.3%}\")\n",
    "\n",
    "    thr_list = [float(t) for t in chosen_thrs]   # <- fixed (was thrs_tt)\n",
    "    print(\"Thresholds:\", [f\"{t:.3f}\" for t in thr_list])\n",
    "\n",
    "    return scores, chosen_thrs, per_fold_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa750f37",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Run Threshold-Tuned CV\n",
    "\n",
    "Using the **time-series folds**, we now:  \n",
    "- Train LightGBM with early stopping  \n",
    "- Tune thresholds per fold to maximize **G-Mean**  \n",
    "- Collect cross-validation scores, thresholds, and stats  \n",
    "\n",
    "This gives us a reliable **local validation estimate** before submitting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe02e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  | GM=0.487 | Thr=0.020 | TPR=0.256 | TNR=0.928 | Pos=481, Neg=25799, PosRatio=1.830% | BestIter=30\n",
      "Fold 2  | GM=0.792 | Thr=0.020 | TPR=0.911 | TNR=0.689 | Pos=895, Neg=25385, PosRatio=3.406% | BestIter=13\n",
      "Fold 3  | GM=0.347 | Thr=0.040 | TPR=0.123 | TNR=0.979 | Pos=756, Neg=25524, PosRatio=2.877% | BestIter=13\n",
      "\n",
      "CV G-Mean: 0.542 Â± 0.186\n",
      "Avg validation positive ratio: 2.704%\n",
      "Thresholds: ['0.020', '0.020', '0.040']\n"
     ]
    }
   ],
   "source": [
    "# --- Run threshold-tuned cross-validation (time-series folds) ---\n",
    "scores_tt, thrs_tt, stats_tt = run_cv_threshold_tuned(\n",
    "    df_feat_train, ts_folds, feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525ed93",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Cross-Validation Results (Time-Series Folds)\n",
    "\n",
    "After running **threshold-tuned cross-validation** with LightGBM, we obtain:  \n",
    "\n",
    "- Per-fold performance metrics (G-Mean, threshold, TPR, TNR)  \n",
    "- Validation class distribution (positive/negative counts and ratio)  \n",
    "- Best boosting iteration selected via early stopping  \n",
    "\n",
    "This provides a clear picture of how well the model generalizes across different time segments.  \n",
    "\n",
    "**Summary:**  \n",
    "- Average CV G-Mean: **0.542 Â± 0.186**  \n",
    "- Average validation positive ratio: **2.704%**  \n",
    "- Chosen thresholds per fold: `['0.020', '0.020', '0.040']`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f6a86",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Run Threshold-Tuned CV (LOSO)\n",
    "\n",
    "Using **leave-one-site-out (LOSO)** folds, we:  \n",
    "- Train on two sites and validate on the **held-out site**  \n",
    "- Tune per-fold thresholds to maximize **G-Mean**  \n",
    "- Assess **cross-site generalization** (how well the model transfers to unseen sites)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d32c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  | GM=0.815 | Thr=0.020 | TPR=0.739 | TNR=0.898 | Pos=771, Neg=34173, PosRatio=2.206% | BestIter=55\n",
      "Fold 2  | GM=0.850 | Thr=0.020 | TPR=0.816 | TNR=0.885 | Pos=1109, Neg=33835, PosRatio=3.174% | BestIter=52\n",
      "Fold 3  | GM=0.049 | Thr=0.040 | TPR=0.002 | TNR=1.000 | Pos=1229, Neg=33715, PosRatio=3.517% | BestIter=2\n",
      "\n",
      "CV G-Mean: 0.571 Â± 0.369\n",
      "Avg validation positive ratio: 2.966%\n",
      "Thresholds: ['0.020', '0.020', '0.040']\n"
     ]
    }
   ],
   "source": [
    "# --- Run threshold-tuned cross-validation (leave-one-site-out folds) ---\n",
    "scores_loso, thrs_loso, stats_loso = run_cv_threshold_tuned(\n",
    "    df_feat_train, loso_folds, feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30629fc",
   "metadata": {},
   "source": [
    "## ðŸŒ Cross-Validation Results (LOSO Folds)\n",
    "\n",
    "After running **threshold-tuned cross-validation** with LightGBM, we obtain:  \n",
    "\n",
    "- Per-fold performance metrics (G-Mean, threshold, TPR, TNR)  \n",
    "- Validation class distribution (positive/negative counts and ratio)  \n",
    "- Best boosting iteration selected via early stopping  \n",
    "\n",
    "This provides a strict test of **site-to-site generalization**, where the model is trained on two sites and validated on the held-out third site.  \n",
    "\n",
    "**Summary:**  \n",
    "- Average CV G-Mean: **0.571 Â± 0.369**  \n",
    "- Average validation positive ratio: **2.966%**  \n",
    "- Chosen thresholds per fold: `['0.020', '0.020', '0.040']`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cd3dc",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸â€â™‚ï¸ Prepare Final Training Matrix\n",
    "\n",
    "Before fitting the full model, we:  \n",
    "- Drop any rows with **NaNs** in features or target (`y_event`)  \n",
    "- Extract the finalized **feature matrix (`X`)** and **target vector (`y`)**  \n",
    "- Print the dataset dimensions to confirm the training setup  \n",
    "\n",
    "This ensures the model is trained on a **clean, consistent dataset** aligned with our CV pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "160a50a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 104,832 | Features: 26\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare training matrix (clean, NaN-free) ---\n",
    "df_train_nn = df_feat_train.dropna(subset=feature_cols_final + [\"y_event\"]).copy()\n",
    "\n",
    "X = df_train_nn[feature_cols_final]\n",
    "y = df_train_nn[\"y_event\"].astype(int)\n",
    "print(f\"Training rows: {len(X):,} | Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33fcc6",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Train Final Model & Generate Submission\n",
    "\n",
    "We now train a **final LightGBM** model on the full training data using the **same settings as cross-validation** to ensure consistency.  \n",
    "\n",
    "- **Class imbalance handling**: `scale_pos_weight = sqrt(neg/pos)`  \n",
    "- **Threshold**: mean of CV-tuned thresholds (fallback = `0.02`)  \n",
    "- **Submission format**:  \n",
    "  - `-1` â†’ event predicted  \n",
    "  - `0` â†’ no event  \n",
    "\n",
    "This produces the final submission file aligned with the competition requirements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d04e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted events in test: 1326 / 35040\n",
      "Using threshold: 0.027 | pos=3109, neg=101723, scale_pos_weight=5.720\n"
     ]
    }
   ],
   "source": [
    "# --- Train final model with CV-consistent settings ---\n",
    "# Class imbalance weight (same recipe as CV): sqrt(neg/pos)\n",
    "pos = max(int((y == 1).sum()), 1)\n",
    "neg = int((y == 0).sum())\n",
    "spw = (neg / pos) ** 0.5\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_data_in_leaf=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=float(spw),\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# --- Predict on test ---\n",
    "X_test = df_feat_test[feature_cols_final]\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Use mean threshold from CV if available; else default to 0.02 ---\n",
    "FIXED_THR = float(np.mean([float(t) for t in thrs_loso])) if \"thrs_loso\" in locals() else 0.02\n",
    "y_pred = (y_prob >= FIXED_THR).astype(int)\n",
    "\n",
    "# --- Map to competition labels: event -> -1, no event -> 0 ---\n",
    "y_flag = np.where(y_pred == 1, -1, 0)\n",
    "\n",
    "# --- Build submission from ORIGINAL test df (preserve Site & Timestamp) ---\n",
    "sub = pd.DataFrame({\n",
    "    \"Site\": df_test[\"Site\"].values,\n",
    "    \"Timestamp_Local\": pd.to_datetime(df_test[\"Timestamp_Local\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Demand_Response_Flag\": y_flag\n",
    "})\n",
    "\n",
    "# --- Quick sanity prints ---\n",
    "print(f\"Predicted events in test: {(sub['Demand_Response_Flag'] != 0).sum()} / {len(sub)}\")\n",
    "print(f\"Using threshold: {FIXED_THR:.3f} | pos={pos}, neg={neg}, scale_pos_weight={spw:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bcfa0",
   "metadata": {},
   "source": [
    "## âœ… Final Model Training & Test Predictions\n",
    "\n",
    "The full model was trained with **CV-consistent settings** (same parameters and imbalance handling as in cross-validation).  \n",
    "\n",
    "**Key outcomes:**  \n",
    "- **Predicted events in test:** 1,326 / 35,040 rows (â‰ˆ **3.8%**)  \n",
    "- **Threshold applied:** 0.027 (mean from CV, more conservative than baseline 0.020)  \n",
    "- **Training distribution:** 3,109 positives vs. 101,723 negatives  \n",
    "- **Class weight (scale_pos_weight):** 5.72  \n",
    "\n",
    "This ensures the **final submission** is aligned with cross-validation insights, balancing event detection against false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7398be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved: ../submissions/submission_2025-09-05_lgbm_loso-cv_v1.csv | shape=(35040, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- Save submission with informative filename ---\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")  # current date\n",
    "model_tag = \"lgbm\"                           # model identifier\n",
    "cv_tag = \"loso-cv\"                             # cross-validation strategy\n",
    "version = \"v1\"                               # manual version tag\n",
    "\n",
    "# Construct filename and path\n",
    "filename = f\"submission_{today}_{model_tag}_{cv_tag}_{version}.csv\"\n",
    "save_path = f\"../submissions/{filename}\"\n",
    "\n",
    "# Ensure folder exists and save CSV\n",
    "#os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "sub.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Submission saved: {save_path} | shape={sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4cf6d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.001 | LOSO CV G-Mean=0.000\n",
      "thr=0.003 | LOSO CV G-Mean=0.000\n",
      "thr=0.005 | LOSO CV G-Mean=0.000\n",
      "thr=0.007 | LOSO CV G-Mean=0.567\n",
      "thr=0.009 | LOSO CV G-Mean=0.578\n",
      "thr=0.011 | LOSO CV G-Mean=0.569\n",
      "thr=0.013 | LOSO CV G-Mean=0.565\n",
      "thr=0.015 | LOSO CV G-Mean=0.562\n",
      "thr=0.018 | LOSO CV G-Mean=0.559\n",
      "thr=0.020 | LOSO CV G-Mean=0.556\n",
      "thr=0.022 | LOSO CV G-Mean=0.552\n",
      "thr=0.024 | LOSO CV G-Mean=0.546\n",
      "thr=0.026 | LOSO CV G-Mean=0.670\n",
      "thr=0.028 | LOSO CV G-Mean=0.663\n",
      "thr=0.030 | LOSO CV G-Mean=0.658\n",
      "\n",
      "Chosen threshold (LOSO): 0.026 | CV G-Mean=0.670\n",
      "Submission saved: ../submissions/submission_2025-09-05_lgbm_loso-cv_thr0.026.csv | events=1336/35040\n"
     ]
    }
   ],
   "source": [
    "# --- Leak-free threshold tuning using LOSO folds (retrain per fold) ---\n",
    "\n",
    "candidate_thrs = np.linspace(0.001, 0.030, 15)  # tighter, low range\n",
    "best_thr, best_cv_gm = None, -1.0\n",
    "\n",
    "for thr in candidate_thrs:\n",
    "    fold_gms = []\n",
    "    for tr_idx, va_idx in loso_folds:\n",
    "        # Per-fold train/valid split + dropna (exactly like CV)\n",
    "        tr = df_feat_train.loc[tr_idx].dropna(subset=feature_cols_final + [\"y_event\"])\n",
    "        va = df_feat_train.loc[va_idx].dropna(subset=feature_cols_final + [\"y_event\"])\n",
    "\n",
    "        X_tr, y_tr = tr[feature_cols_final], tr[\"y_event\"]\n",
    "        X_va, y_va = va[feature_cols_final], va[\"y_event\"]\n",
    "\n",
    "        # Same imbalance weighting as CV: sqrt(neg/pos)\n",
    "        pos = max((y_tr == 1).sum(), 1)\n",
    "        neg = (y_tr == 0).sum()\n",
    "        spw = (neg / pos) ** 0.5\n",
    "\n",
    "        # Same model params as CV\n",
    "        clf_cv = LGBMClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=127,\n",
    "            max_depth=-1,\n",
    "            min_data_in_leaf=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            scale_pos_weight=float(spw),\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=150, verbose=False),\n",
    "            lgb.log_evaluation(period=0),\n",
    "        ]\n",
    "        clf_cv.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Predict VA and compute G-Mean at the *single* candidate threshold\n",
    "        y_prob_va = clf_cv.predict_proba(X_va, num_iteration=clf_cv.best_iteration_)[:, 1]\n",
    "        gm, _, _, _ = gmean_from_probs(y_va, y_prob_va, [thr])\n",
    "        fold_gms.append(gm)\n",
    "\n",
    "    avg_gm = float(np.mean(fold_gms))\n",
    "    print(f\"thr={thr:.3f} | LOSO CV G-Mean={avg_gm:.3f}\")\n",
    "    if avg_gm > best_cv_gm:\n",
    "        best_cv_gm, best_thr = avg_gm, thr\n",
    "\n",
    "print(f\"\\nChosen threshold (LOSO): {best_thr:.3f} | CV G-Mean={best_cv_gm:.3f}\")\n",
    "\n",
    "# --- Apply the chosen threshold to your already-trained full model ---\n",
    "y_pred = (y_prob >= best_thr).astype(int)          # y_prob came from the full-model on test\n",
    "y_flag = np.where(y_pred == 1, -1, 0)\n",
    "sub[\"Demand_Response_Flag\"] = y_flag\n",
    "\n",
    "# Save tuned submission\n",
    "from datetime import datetime\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = f\"submission_{today}_lgbm_loso-cv_thr{best_thr:.3f}.csv\"\n",
    "save_path = f\"../submissions/{filename}\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "sub.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Submission saved: {save_path} | events={int((sub['Demand_Response_Flag']!=0).sum())}/{len(sub)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
