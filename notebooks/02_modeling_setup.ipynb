{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6b2334",
   "metadata": {},
   "source": [
    "# 🧩 FlexTrack Challenge — Modeling Setup\n",
    "\n",
    "**Notebook:** 02_modeling_setup.ipynb  \n",
    "**Goal:** Establish a robust modeling pipeline starting with  \n",
    "time-aware, site-aware cross-validation and a baseline classifier.  \n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Objectives\n",
    "- Implement a **cross-validation strategy** that prevents leakage:\n",
    "  - Time-based splits within each site\n",
    "  - Optional leave-one-site-out for robustness\n",
    "- Verify split integrity (train always before validation, no overlap)\n",
    "- Train and evaluate a **baseline classifier** (LightGBM/XGBoost) with class weights\n",
    "- Prepare foundation for Phase 2 (regression)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Context Recap\n",
    "- **Target variable (Phase 1):** `Demand_Response_Flag` (-1, 0, +1)  \n",
    "- **Target variable (Phase 2):** `Demand_Response_Capacity_kW` (continuous, when flag ≠ 0)  \n",
    "- **EDA findings:**  \n",
    "  - Severe class imbalance (~97% zeros)  \n",
    "  - Events occur in business hours, hot/sunny conditions  \n",
    "  - Building power shows strong autocorrelation  \n",
    "- **Feature blueprint:**  \n",
    "  - Time, weather, load dynamics, site identity, persistence, interactions  \n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Next Steps\n",
    "1. Define and test **time-aware CV splitters**\n",
    "2. Evaluate **baseline classifier** performance across folds\n",
    "3. Store CV framework for downstream feature engineering and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3874cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# --- Core imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Modeling ---\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Utilities ---\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Environment ready. Seed set to\", SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff3603a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/processed/flextrack_train.csv\"\n",
    "df_train = pd.read_csv(train_path, parse_dates=[\"Timestamp_Local\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e8f9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_site_timeseries_folds(df, n_splits=3):\n",
    "    \"\"\"\n",
    "    Creates n_splits folds where each fold is a union of\n",
    "    time-ordered splits made independently within each site.\n",
    "    Assumes df is already sorted by ['Site','Timestamp_Local'].\n",
    "    Returns: list of (train_idx, val_idx) tuples (as numpy arrays).\n",
    "    \"\"\"\n",
    "    folds_per_site = {}\n",
    "    for site, dsite in df.groupby(\"Site\", sort=False):\n",
    "        idx = dsite.index.to_numpy()\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        site_folds = []\n",
    "        for tr_rel, va_rel in tscv.split(idx):\n",
    "            site_folds.append((idx[tr_rel], idx[va_rel]))\n",
    "        folds_per_site[site] = site_folds\n",
    "\n",
    "    folds = []\n",
    "    for k in range(n_splits):\n",
    "        tr_parts, va_parts = [], []\n",
    "        for site in folds_per_site:\n",
    "            tr_idx, va_idx = folds_per_site[site][k]\n",
    "            tr_parts.append(tr_idx)\n",
    "            va_parts.append(va_idx)\n",
    "        folds.append((np.concatenate(tr_parts), np.concatenate(va_parts)))\n",
    "    return folds\n",
    "\n",
    "def make_leave_one_site_out_folds(df):\n",
    "    \"\"\"\n",
    "    Strict generalization check: train on two sites, validate on the third.\n",
    "    Returns: list of (train_idx, val_idx) tuples.\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    all_idx = df.index.to_numpy()\n",
    "    for site, dsite in df.groupby(\"Site\", sort=False):\n",
    "        val_idx = dsite.index.to_numpy()\n",
    "        train_mask = np.isin(all_idx, val_idx, invert=True)\n",
    "        tr_idx = all_idx[train_mask]\n",
    "        folds.append((tr_idx, val_idx))\n",
    "    return folds\n",
    "\n",
    "# 🔎 Quick integrity check helper\n",
    "def summarize_fold(df, tr_idx, va_idx, label=\"\"):\n",
    "    tr = df.loc[tr_idx]\n",
    "    va = df.loc[va_idx]\n",
    "    print(f\"{label} | Train: {tr.shape[0]:>6} rows  [{tr['Timestamp_Local'].min()} → {tr['Timestamp_Local'].max()}]\")\n",
    "    print(f\"{' '*len(label)} | Valid: {va.shape[0]:>6} rows  [{va['Timestamp_Local'].min()} → {va['Timestamp_Local'].max()}]\")\n",
    "    # Ensure no temporal leakage within each site\n",
    "    for s in df['Site'].unique():\n",
    "        tr_s = tr[tr['Site']==s]\n",
    "        va_s = va[va['Site']==s]\n",
    "        if not tr_s.empty and not va_s.empty:\n",
    "            assert tr_s['Timestamp_Local'].max() < va_s['Timestamp_Local'].min(), f\"Leakage in site {s}\"\n",
    "\n",
    "# ▶️ Build folds \n",
    "ts_folds = make_site_timeseries_folds(df_train, n_splits=3)   # primary\n",
    "loso_folds = make_leave_one_site_out_folds(df_train)          # secondary (robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8c2a4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS Fold 1 | Train:  26280 rows  [2019-01-01 00:00:00 → 2023-04-02 05:45:00]\n",
      "          | Valid:  26280 rows  [2019-04-02 06:00:00 → 2023-07-02 11:45:00]\n",
      "TS Fold 2 | Train:  52560 rows  [2019-01-01 00:00:00 → 2023-07-02 11:45:00]\n",
      "          | Valid:  26280 rows  [2019-07-02 12:00:00 → 2023-10-01 17:45:00]\n",
      "TS Fold 3 | Train:  78840 rows  [2019-01-01 00:00:00 → 2023-10-01 17:45:00]\n",
      "          | Valid:  26280 rows  [2019-10-01 18:00:00 → 2023-12-31 23:45:00]\n",
      "LOSO Fold 1 | Train:  70080 rows  [2019-01-01 00:00:00 → 2023-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2019-01-01 00:00:00 → 2019-12-31 23:45:00]\n",
      "LOSO Fold 2 | Train:  70080 rows  [2019-01-01 00:00:00 → 2023-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2019-01-01 00:00:00 → 2019-12-31 23:45:00]\n",
      "LOSO Fold 3 | Train:  70080 rows  [2019-01-01 00:00:00 → 2019-12-31 23:45:00]\n",
      "            | Valid:  35040 rows  [2023-01-01 00:00:00 → 2023-12-31 23:45:00]\n"
     ]
    }
   ],
   "source": [
    "# Summarize all time-series folds\n",
    "for i, (tr_idx, va_idx) in enumerate(ts_folds, 1):\n",
    "    summarize_fold(df_train, tr_idx, va_idx, label=f\"TS Fold {i}\")\n",
    "\n",
    "# Summarize all LOSO folds (strict generalization)\n",
    "for i, (tr_idx, va_idx) in enumerate(loso_folds, 1):\n",
    "    summarize_fold(df_train, tr_idx, va_idx, label=f\"LOSO Fold {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a378c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df):\n",
    "    d = df.copy()\n",
    "\n",
    "    # ensure datetime + sorted\n",
    "    if not pd.api.types.is_datetime64_any_dtype(d[\"Timestamp_Local\"]):\n",
    "        d[\"Timestamp_Local\"] = pd.to_datetime(d[\"Timestamp_Local\"], errors=\"raise\")\n",
    "    d = d.sort_values([\"Site\",\"Timestamp_Local\"])\n",
    "\n",
    "    # target (only if available)\n",
    "    if \"Demand_Response_Flag\" in d.columns:\n",
    "        d[\"y_event\"] = (d[\"Demand_Response_Flag\"] != 0).astype(int)\n",
    "    else:\n",
    "        d[\"y_event\"] = np.nan  # test data has no labels\n",
    "\n",
    "    # time\n",
    "    d[\"hour\"]   = d[\"Timestamp_Local\"].dt.hour\n",
    "    d[\"dow\"]    = d[\"Timestamp_Local\"].dt.dayofweek\n",
    "    d[\"month\"]  = d[\"Timestamp_Local\"].dt.month\n",
    "    d[\"bizhrs\"] = d[\"hour\"].between(10,17).astype(int)\n",
    "    d[\"wknd\"]   = d[\"dow\"].isin([5,6]).astype(int)\n",
    "\n",
    "    # weather\n",
    "    d[\"temp\"] = d[\"Dry_Bulb_Temperature_C\"]\n",
    "    d[\"rad\"]  = d[\"Global_Horizontal_Radiation_W/m2\"]\n",
    "    d[\"temp_x_rad\"] = d[\"temp\"] * d[\"rad\"]\n",
    "\n",
    "    # load lags / rolling (per-site)\n",
    "    for lag in [1, 4, 12, 24, 96]:\n",
    "        d[f\"power_lag{lag}\"] = d.groupby(\"Site\")[\"Building_Power_kW\"].shift(lag)\n",
    "\n",
    "    d[\"power_roll4\"]  = d.groupby(\"Site\")[\"Building_Power_kW\"].transform(lambda s: s.rolling(4,  min_periods=1).mean())\n",
    "    d[\"power_roll12\"] = d.groupby(\"Site\")[\"Building_Power_kW\"].transform(lambda s: s.rolling(12, min_periods=1).mean())\n",
    "    d[\"power_roll96\"] = d.groupby(\"Site\")[\"Building_Power_kW\"].transform(lambda s: s.rolling(96, min_periods=1).mean())\n",
    "    d[\"power_std96\"]  = d.groupby(\"Site\")[\"Building_Power_kW\"].transform(lambda s: s.rolling(96, min_periods=10).std())\n",
    "\n",
    "    # deltas\n",
    "    d[\"power_delta\"] = d[\"Building_Power_kW\"] - d[\"power_lag1\"]\n",
    "\n",
    "    # per-site z-score\n",
    "    d[\"power_z\"] = d.groupby(\"Site\")[\"Building_Power_kW\"].transform(\n",
    "        lambda s: (s - s.mean()) / (s.std() + 1e-6)\n",
    "    )\n",
    "\n",
    "    # site one-hot\n",
    "    d = pd.get_dummies(d, columns=[\"Site\"], drop_first=False)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2087b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (feat): (105120, 29)\n",
      "Test  shape (feat): (35040, 27)\n",
      "Num features: 22\n"
     ]
    }
   ],
   "source": [
    "# --- paths (adjust if needed) ---\n",
    "train_path = \"../data/processed/flextrack_train.csv\"      # or your processed path if you saved one\n",
    "test_path  = \"../data/raw/flextrack-public-test-data-v0.1.csv\"       # <- official warm-up test set\n",
    "\n",
    "# --- load (parse timestamp) ---\n",
    "df_train = pd.read_csv(train_path, parse_dates=[\"Timestamp_Local\"])\n",
    "df_test  = pd.read_csv(test_path,  parse_dates=[\"Timestamp_Local\"])\n",
    "\n",
    "# --- build features for train & test ---\n",
    "df_feat_train = build_features(df_train)\n",
    "df_feat_test  = build_features(df_test)\n",
    "\n",
    "# --- align one-hot columns (Site_*) between train and test ---\n",
    "train_sites = [c for c in df_feat_train.columns if c.startswith(\"Site_\")]\n",
    "test_sites  = [c for c in df_feat_test.columns  if c.startswith(\"Site_\")]\n",
    "\n",
    "# add any missing site dummies to test\n",
    "for missing in set(train_sites) - set(test_sites):\n",
    "    df_feat_test[missing] = 0\n",
    "# drop extra site dummies in test (shouldn't happen, but safe)\n",
    "for extra in set(test_sites) - set(train_sites):\n",
    "    df_feat_test.drop(columns=[extra], inplace=True)\n",
    "\n",
    "# --- define feature list (same as our best CV run) ---\n",
    "feature_cols = [\n",
    "    \"hour\",\"dow\",\"month\",\"bizhrs\",\"wknd\",\n",
    "    \"temp\",\"rad\",\"temp_x_rad\",\n",
    "    \"power_lag1\",\"power_lag4\",\"power_lag12\",\"power_lag24\",\"power_lag96\",\n",
    "    \"power_roll4\",\"power_roll12\",\"power_roll96\",\"power_std96\",\n",
    "    \"power_delta\",\"power_z\",\n",
    "] + [c for c in df_feat_train.columns if c.startswith(\"Site_\")]\n",
    "\n",
    "# keep only features present in BOTH train & test (after alignment)\n",
    "feature_cols_final = [c for c in feature_cols if c in df_feat_train.columns and c in df_feat_test.columns]\n",
    "\n",
    "print(\"Train shape (feat):\", df_feat_train.shape)\n",
    "print(\"Test  shape (feat):\", df_feat_test.shape)\n",
    "print(\"Num features:\", len(feature_cols_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fec2e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean_binary(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp+fn) > 0 else 0.0   # sensitivity (recall for events)\n",
    "    tnr = tn / (tn + fp) if (tn+fp) > 0 else 0.0   # specificity (non-events)\n",
    "    return np.sqrt(tpr * tnr), tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bfd0390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 977, number of negative: 25015\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3361\n",
      "[LightGBM] [Info] Number of data points in the train set: 25992, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 1: G-Mean=0.0000 (TPR=0.000, TNR=1.000)\n",
      "[LightGBM] [Info] Number of positive: 1458, number of negative: 50814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3364\n",
      "[LightGBM] [Info] Number of data points in the train set: 52272, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 2: G-Mean=0.4948 (TPR=0.264, TNR=0.928)\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 76199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3367\n",
      "[LightGBM] [Info] Number of data points in the train set: 78552, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Fold 3: G-Mean=0.0000 (TPR=0.000, TNR=1.000)\n",
      "\n",
      "CV G-Mean: mean=0.1649, std=0.2332\n"
     ]
    }
   ],
   "source": [
    "def run_cv_baseline(df_feat, folds, feature_cols):\n",
    "    scores = []\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        tr = df_feat.loc[tr_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "        va = df_feat.loc[va_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "\n",
    "        X_tr, y_tr = tr[feature_cols], tr[\"y_event\"]\n",
    "        X_va, y_va = va[feature_cols], va[\"y_event\"]\n",
    "\n",
    "        # handle imbalance → weight positive (event) class\n",
    "        pos_weight = (y_tr==0).sum() / max((y_tr==1).sum(), 1)\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            class_weight={0:1.0, 1:float(pos_weight)}\n",
    "        )\n",
    "        clf.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = clf.predict(X_va)\n",
    "        gm, tpr, tnr = gmean_binary(y_va, y_pred)\n",
    "        scores.append(gm)\n",
    "\n",
    "        print(f\"Fold {i}: G-Mean={gm:.4f} (TPR={tpr:.3f}, TNR={tnr:.3f})\")\n",
    "\n",
    "    print(f\"\\nCV G-Mean: mean={np.mean(scores):.4f}, std={np.std(scores):.4f}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ▶️ Run with your time-series folds\n",
    "scores = run_cv_baseline(df_feat, ts_folds, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfa16a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean_from_probs(y_true, y_prob, thresholds):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      best_gm, best_thr, best_tpr, best_tnr\n",
    "    \"\"\"\n",
    "    best = (-1.0, 0.5, 0.0, 0.0)  # (gm, thr, tpr, tnr)\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "        tpr = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "        tnr = tn / (tn + fp) if (tn+fp)>0 else 0.0\n",
    "        gm  = np.sqrt(tpr * tnr)\n",
    "        if gm > best[0]:\n",
    "            best = (gm, thr, tpr, tnr)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09951ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_threshold_tuned(df_feat, folds, feature_cols, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.02, 0.5, 25)\n",
    "\n",
    "    scores, chosen_thrs, per_fold_stats = [], [], []\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        tr = df_feat.loc[tr_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "        va = df_feat.loc[va_idx].dropna(subset=feature_cols + [\"y_event\"])\n",
    "\n",
    "        X_tr, y_tr = tr[feature_cols], tr[\"y_event\"]\n",
    "        X_va, y_va = va[feature_cols], va[\"y_event\"]\n",
    "\n",
    "        # 🔹 softer imbalance handling: sqrt(neg/pos)\n",
    "        pos = max((y_tr == 1).sum(), 1)\n",
    "        neg = (y_tr == 0).sum()\n",
    "        spw = (neg / pos) ** 0.5\n",
    "\n",
    "        clf = LGBMClassifier(\n",
    "            random_state=42,\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=127,\n",
    "            max_depth=-1,\n",
    "            min_data_in_leaf=20,     # was 50\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            scale_pos_weight=float(spw),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        callbacks = [lgb.early_stopping(stopping_rounds=150, verbose=False)]\n",
    "        clf.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # probabilities (use best iteration if present)\n",
    "        try:\n",
    "            y_prob = clf.predict_proba(X_va, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        except Exception:\n",
    "            y_prob = clf.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        # threshold sweep → best G-Mean\n",
    "        best_gm, best_thr, best_tpr, best_tnr = gmean_from_probs(y_va, y_prob, thresholds)\n",
    "        scores.append(best_gm)\n",
    "        chosen_thrs.append(best_thr)\n",
    "        per_fold_stats.append((best_tpr, best_tnr))\n",
    "\n",
    "        bi = getattr(clf, \"best_iteration_\", None)\n",
    "        print(\n",
    "            f\"Fold {i}: G-Mean={best_gm:.4f} @ thr={best_thr:.3f} \"\n",
    "            f\"(TPR={best_tpr:.3f}, TNR={best_tnr:.3f})\"\n",
    "            + (f\" | best_iter={bi}\" if bi is not None else \"\")\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCV G-Mean: mean={np.mean(scores):.4f}, std={np.std(scores):.4f}\")\n",
    "    print(f\"Chosen thresholds per fold: {[round(t,3) for t in chosen_thrs]}\")\n",
    "    return scores, chosen_thrs, per_fold_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfe02e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 25015\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3616\n",
      "[LightGBM] [Info] Number of data points in the train set: 25992, number of used features: 22\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037588 -> initscore=-3.242744\n",
      "[LightGBM] [Info] Start training from score -3.242744\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 1: G-Mean=0.4406 @ thr=0.020 (TPR=0.208, TNR=0.934) | best_iter=27\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 1458, number of negative: 50814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3619\n",
      "[LightGBM] [Info] Number of data points in the train set: 52272, number of used features: 22\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027893 -> initscore=-3.551106\n",
      "[LightGBM] [Info] Start training from score -3.551106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 2: G-Mean=0.7878 @ thr=0.040 (TPR=0.750, TNR=0.828) | best_iter=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 76199\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3622\n",
      "[LightGBM] [Info] Number of data points in the train set: 78552, number of used features: 22\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029955 -> initscore=-3.477657\n",
      "[LightGBM] [Info] Start training from score -3.477657\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 3: G-Mean=0.7604 @ thr=0.020 (TPR=0.697, TNR=0.829) | best_iter=19\n",
      "\n",
      "CV G-Mean: mean=0.6630, std=0.1576\n",
      "Chosen thresholds per fold: [np.float64(0.02), np.float64(0.04), np.float64(0.02)]\n"
     ]
    }
   ],
   "source": [
    "scores_tt, thrs_tt, stats_tt = run_cv_threshold_tuned(df_feat, ts_folds, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36d32c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 2338, number of negative: 67550\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3622\n",
      "[LightGBM] [Info] Number of data points in the train set: 69888, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033454 -> initscore=-3.363572\n",
      "[LightGBM] [Info] Start training from score -3.363572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 1: G-Mean=0.8871 @ thr=0.020 (TPR=0.907, TNR=0.868) | best_iter=64\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 2000, number of negative: 67888\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3622\n",
      "[LightGBM] [Info] Number of data points in the train set: 69888, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.028617 -> initscore=-3.524712\n",
      "[LightGBM] [Info] Start training from score -3.524712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 2: G-Mean=0.8959 @ thr=0.020 (TPR=0.912, TNR=0.880) | best_iter=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 1880, number of negative: 68008\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3622\n",
      "[LightGBM] [Info] Number of data points in the train set: 69888, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026900 -> initscore=-3.588354\n",
      "[LightGBM] [Info] Start training from score -3.588354\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Fold 3: G-Mean=0.7793 @ thr=0.020 (TPR=0.771, TNR=0.787) | best_iter=70\n",
      "\n",
      "CV G-Mean: mean=0.8541, std=0.0530\n",
      "Chosen thresholds per fold: [np.float64(0.02), np.float64(0.02), np.float64(0.02)]\n"
     ]
    }
   ],
   "source": [
    "scores_loso, thrs_loso, stats_loso = run_cv_threshold_tuned(df_feat, loso_folds, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4dc2baf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Info] Number of positive: 3109, number of negative: 102011\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3624\n",
      "[LightGBM] [Info] Number of data points in the train set: 105120, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029576 -> initscore=-3.490780\n",
      "[LightGBM] [Info] Start training from score -3.490780\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "Predicted events in test: 1649 / 35040\n",
      "Submission saved: ../submissions/submission_phase1.csv | shape: (35040, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Timestamp_Local</th>\n",
       "      <th>Demand_Response_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siteD</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siteD</td>\n",
       "      <td>2020-01-01 00:15:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siteD</td>\n",
       "      <td>2020-01-01 00:30:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siteD</td>\n",
       "      <td>2020-01-01 00:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siteD</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Site      Timestamp_Local  Demand_Response_Flag\n",
       "0  siteD  2020-01-01 00:00:00                     0\n",
       "1  siteD  2020-01-01 00:15:00                     0\n",
       "2  siteD  2020-01-01 00:30:00                     0\n",
       "3  siteD  2020-01-01 00:45:00                     0\n",
       "4  siteD  2020-01-01 01:00:00                     0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Train full model ---\n",
    "X = df_feat_train[feature_cols_final]\n",
    "y = (df_feat_train[\"Demand_Response_Flag\"] != 0).astype(int)\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=20,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# --- Predict on test ---\n",
    "X_test = df_feat_test[feature_cols_final]\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# apply tuned threshold (0.02)\n",
    "FIXED_THR = 0.02\n",
    "y_pred = (y_prob >= FIXED_THR).astype(int)\n",
    "\n",
    "# For warm-up: submit event vs no-event. We'll emit -1 for events, 0 otherwise.\n",
    "y_flag = np.where(y_pred == 1, -1, 0)\n",
    "\n",
    "# --- Build submission using the ORIGINAL test df (has Site & Timestamp) ---\n",
    "sub = pd.DataFrame({\n",
    "    \"Site\": df_test[\"Site\"].values,\n",
    "    \"Timestamp_Local\": pd.to_datetime(df_test[\"Timestamp_Local\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Demand_Response_Flag\": y_flag\n",
    "})\n",
    "\n",
    "# optional sanity check\n",
    "print(\"Predicted events in test:\", (sub[\"Demand_Response_Flag\"] != 0).sum(), \"/\", len(sub))\n",
    "\n",
    "# --- Save to CSV ---\n",
    "save_path = \"../submissions/submission_phase1.csv\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "sub.to_csv(save_path, index=False)\n",
    "print(\"Submission saved:\", save_path, \"| shape:\", sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "54c31b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.003 | events=2922/35040 | saved -> ../submissions/submission_phase1_thr0p003.csv\n",
      "thr=0.004 | events=2683/35040 | saved -> ../submissions/submission_phase1_thr0p004.csv\n",
      "thr=0.005 | events=2529/35040 | saved -> ../submissions/submission_phase1_thr0p005.csv\n"
     ]
    }
   ],
   "source": [
    "def write_submission_from_probs(y_prob, thr, df_test, path):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    # Warm-up phase: treat any event as -1 (binary evaluation)\n",
    "    y_flag = np.where(y_pred == 1, -1, 0)\n",
    "    sub = pd.DataFrame({\n",
    "        \"Site\": df_test[\"Site\"].values,\n",
    "        \"Timestamp_Local\": pd.to_datetime(df_test[\"Timestamp_Local\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"Demand_Response_Flag\": y_flag\n",
    "    })\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    sub.to_csv(path, index=False)\n",
    "    print(f\"thr={thr:0.3f} | events={int((sub['Demand_Response_Flag']!=0).sum())}/{len(sub)} | saved -> {path}\")\n",
    "\n",
    "# thresholds to try (tighter around your current best)\n",
    "ths = [0.003, 0.004, 0.005]\n",
    "\n",
    "for thr in ths:\n",
    "    out = f\"../submissions/submission_phase1_thr{str(thr).replace('.','p')}.csv\"\n",
    "    write_submission_from_probs(y_prob, thr, df_test, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c098c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
